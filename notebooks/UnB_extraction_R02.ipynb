{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448af9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup # pip install beautifulsoup4\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ad9cf8",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf959fd",
   "metadata": {},
   "source": [
    "### 1.1 Define Initial URL\n",
    "\n",
    "Initially, we will use a URL that seems to lead us to a generic search engine for all theses, dissertations, and post-doctoral products sorted by title.\n",
    "\n",
    "Although the initial requirement was to extract information by sections or research fields (e.g., Political Science), since the page structure is always the same, the idea is to generate code that extracts information based on the page structure rather than the specific filter applied to that page, such as filtering only a certain type of publication by topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfc792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://repositorio2.unb.br/jspui/handle/10482/45731/browse?type=title&sort_by=1&order=ASC&rpp=100&etal=-1&null=&\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb1332",
   "metadata": {},
   "source": [
    "### 1.2 Workflow\n",
    "\n",
    "The page has the following structure (**search page**):\n",
    "\n",
    "- **Table**: Contains rows where each row represents a publication.\n",
    "- **Fields**: Each row displays several fields related to the article, including the title and a link to the specific article page (**article page**).\n",
    "- **Pagination**: You can limit the number of entries shown per page. In this case, we have set it to show the maximum number of results per page, which is **100**.\n",
    "- **Navigation**: At the top and bottom of the table, there are \"Next\" and \"Previous\" buttons to navigate through the pages of search results.\n",
    "\n",
    "### Extraction Flow\n",
    "\n",
    "1. **Start on the Initial Search Page**:\n",
    "   - Begin at the starting search page, which shows articles from 1 to 100 (**start_function**).\n",
    "\n",
    "2. **Find and Collect Article Links**:\n",
    "   - Locate all rows/articles (100) on the page.\n",
    "   - For each row, find the HTML element that references the title and contains a hyperlink to the article's specific page.\n",
    "\n",
    "3. **Visit Each Article Page**:\n",
    "   - For each article/link found:\n",
    "     - Access the hyperlink and go to the corresponding article page (**article page**).\n",
    "     - Extract the information from the visited article.\n",
    "     - Return to the initial search page.\n",
    "\n",
    "4. **Move to the Next Page**:\n",
    "   - After extracting information from all 100 articles, go to the next page.\n",
    "   - Repeat steps 2 and 3 for the new page.\n",
    "\n",
    "5. **End of Extraction**:\n",
    "   - When the \"Next\" button no longer appears, it indicates that you have reached the last search page and have \"scanned\" all relevant articles. This means the task is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a439805",
   "metadata": {},
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42036388",
   "metadata": {},
   "source": [
    "### 2.1. Start Function\n",
    "\n",
    "We create a function to initialize the driver with a given URL.\n",
    "\n",
    "It is important to note that we have configured the driver to block downloads. This is because, in practice, running the code automatically started downloading the documents (PDFs of the publications). \n",
    "\n",
    "To prevent this, we configured the options to block automatic downloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a385f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def open_url_block_dw(url):\n",
    "    \"\"\"\n",
    "    Opens a specified URL in a Chrome browser instance with download restrictions.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL to open in the browser.\n",
    "\n",
    "    Returns:\n",
    "    webdriver.Chrome: The initialized Chrome WebDriver instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configure ChromeOptions to block downloads\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "        \"download.prompt_for_download\": False,  # Disable the download prompt dialog\n",
    "        \"download_restrictions\": 3  # Block all automatic downloads\n",
    "    })\n",
    "\n",
    "    # Initialize the Chrome WebDriver with the specified options\n",
    "    driver = webdriver.Chrome(service=Service(), options=chrome_options)\n",
    "    \n",
    "    # Navigate to the specified URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    return driver\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1c9a1",
   "metadata": {},
   "source": [
    "### 2. Movement Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc3496a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLO SACAMOS AL MAIN, CREO QUE QUEDA MAS CLARO \\n\\ndef page_movement(driver):\\n    \\n    n = 1\\n    \\n    header = driver.find_element(By.CLASS_NAME, \"panel-heading.text-center\").text\\n    \\n    while \"Siguiente\" in header:\\n        \\n        print(f\"pagina {n}\")\\n        \\n        # ejecuto target papers\\n        target_papers(driver)\\n        \\n        driver = next_page(driver)\\n        \\n        n +=1\\n        header = driver.find_element(By.CLASS_NAME, \"panel-heading.text-center\").text\\n        \\n    print(\"Se han escaneado todas las páginas\")\\n    \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "LO SACAMOS AL MAIN, CREO QUE QUEDA MAS CLARO \n",
    "\n",
    "def page_movement(driver):\n",
    "    \n",
    "    n = 1\n",
    "    \n",
    "    header = driver.find_element(By.CLASS_NAME, \"panel-heading.text-center\").text\n",
    "    \n",
    "    while \"Siguiente\" in header:\n",
    "        \n",
    "        print(f\"pagina {n}\")\n",
    "        \n",
    "        # ejecuto target papers\n",
    "        target_papers(driver)\n",
    "        \n",
    "        driver = next_page(driver)\n",
    "        \n",
    "        n +=1\n",
    "        header = driver.find_element(By.CLASS_NAME, \"panel-heading.text-center\").text\n",
    "        \n",
    "    print(\"Se han escaneado todas las páginas\")\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba34a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def next_page(driver):\n",
    "    \"\"\"\n",
    "    Clicks the 'next' button to navigate to the next page in the pagination.\n",
    "\n",
    "    Parameters:\n",
    "    driver (webdriver.Chrome): The Chrome WebDriver instance used to interact with the web page.\n",
    "\n",
    "    Returns:\n",
    "    webdriver.Chrome: The updated Chrome WebDriver instance after navigating to the next page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Locate the footer section where the pagination controls are located\n",
    "    footer = driver.find_element(By.CLASS_NAME, \"panel-footer.text-center\")\n",
    "    \n",
    "    # Find and click the 'next' button, which is typically located in the footer section\n",
    "    footer.find_element(By.CLASS_NAME, \"pull-right\").click()\n",
    "    \n",
    "    return driver\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0cc3b4",
   "metadata": {},
   "source": [
    "### 3. Taget_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3752afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "def target_papers(driver):\n",
    "    \"\"\"\n",
    "    Extracts information from a list of paper links on a webpage and returns it as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    driver (webdriver.Chrome): The Chrome WebDriver instance used to interact with the web page.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - webdriver.Chrome: The updated Chrome WebDriver instance after processing all papers.\n",
    "        - pd.DataFrame: A DataFrame containing the extracted information from the papers.\n",
    "    \"\"\"\n",
    "    \n",
    "    info_list = []\n",
    "    \n",
    "    # Find all elements containing titles/links to published articles\n",
    "    elements = driver.find_elements(By.XPATH, '//td[@headers=\"t3\"]')\n",
    "    \n",
    "    # Loop over the paper elements to extract relevant information\n",
    "    for e in elements:\n",
    "        \n",
    "        # Target the element that contains the link to the paper page\n",
    "        href = e.find_element(By.TAG_NAME, \"a\")\n",
    "    \n",
    "        # Wait for the link to be clickable and then click it to navigate to the paper's page\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable(href)).click()\n",
    "        \n",
    "        # Extract relevant information from the paper's page\n",
    "        info = extract_info(driver)\n",
    "        \n",
    "        # Append the extracted information to the list\n",
    "        info_list.append(info)\n",
    "\n",
    "        # Navigate back to the search results page\n",
    "        driver.back()\n",
    "        \n",
    "    # Convert the list of information dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(info_list)\n",
    "    \n",
    "    return driver, df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539ba0a",
   "metadata": {},
   "source": [
    "# 4. Extraer informacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1452f6d",
   "metadata": {},
   "source": [
    "## Data Structure:\n",
    "\n",
    "All the information relevant to an article is stored in a unique web page, we will call this web page \"article_page\". \n",
    "\n",
    "Inside de page, we can find the link to the document (Ej: PDF document) and a some related fields to the article, that are contained in a table:\n",
    "\n",
    "Table Structure:\n",
    "\n",
    " - First Column: Contains the field name (e.g., \"Title\", \"Authors\").\n",
    "\n",
    " - Second Column: Contains the field content (e.g., \"A very cool dissertation\", \"Author 1, Author 2\").\n",
    "\n",
    "Each row of the table represents a field and its corresponding content. The field name is in the first cell (<td>), and the field content is in the second cell (<td>).\n",
    "    \n",
    "**NOTE:** The table can contain either a single entry or multiple entries. For example:\n",
    "\n",
    "    - A title is unique (there is only one title).\n",
    "    - Authors can be multiple people.\n",
    "    - Topics can include more than one.\n",
    "\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "The goal is to extract specific data from an HTML page using Python. The process involves retrieving HTML content, parsing it, and then extracting structured information from a table.\n",
    "\n",
    "Steps\n",
    "\n",
    "1. **Retrieve HTML Code**, Obtain the HTML source from the current page using the driver object.\n",
    "\n",
    "2. **Parse HTML Content**, Use BeautifulSoup to parse the HTML, enabling easy navigation and manipulation of the document.\n",
    "\n",
    "3. **Locate the table** within the parsed HTML that contains the desired information.\n",
    "\n",
    "4. **Iterate Through Table Rows** Extract data from each row of the table, focusing on fields and their corresponding content.\n",
    "    \n",
    "Each row in the table has two main elements (stored in columns) \n",
    "    Field Name: Located in the first cell (<td> element).\n",
    "    Field Content: Located in the second cell (<td> element).\n",
    "    \n",
    "Field names and contents are to be processed and stored in a dictionary.\n",
    "        \n",
    "5. **Extract and Format Data:**\n",
    "\n",
    "    Field Name: Clean and format the text to use as a dictionary key.\n",
    "    Field Content: Extract text from the field, including handling multiple values (e.g., links)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394cdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_info(driver):\n",
    "    \"\"\"\n",
    "    Extracts relevant information from the current page of a paper.\n",
    "\n",
    "    Parameters:\n",
    "    driver (webdriver.Chrome): The Chrome WebDriver instance used to interact with the web page.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the extracted information from the paper page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the HTML code from the current page (article page)\n",
    "    page = driver.page_source\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup for easy manipulation\n",
    "    info = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    # Create an empty dictionary to store all extracted values\n",
    "    data_dictionary = {}\n",
    "\n",
    "    # Store the document link\n",
    "    try:\n",
    "        # Construct the document link from the current URL and the text in the specified table cell\n",
    "        data_dictionary[\"Document\"] = f\"{driver.current_url}/1/{info.find('td', headers='t1').text}\"\n",
    "    except:\n",
    "        # If the link or text is not found, set a default message\n",
    "        data_dictionary[\"Document\"] = \"No files associated with this item\"\n",
    "\n",
    "    \"\"\"\n",
    "    IMPORTANT NOTE:\n",
    "    What if there is more than one document link?\n",
    "    The link seems to be composed of the article link + /1/ + the name,\n",
    "    which is why it's important to consider multiple links if they exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the table where all the information is contained\n",
    "    table = info.find(\"table\", class_=\"table itemDisplayTable\")\n",
    "\n",
    "    # Find all rows inside the table\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    # Iterate through all rows to get field names and field contents\n",
    "    for r in rows:\n",
    "\n",
    "        # Find data inside each row (includes field_name and field_content)\n",
    "        data = r.find_all(\"td\")  # Each row is composed of two columns or cells\n",
    "\n",
    "        if len(data) < 2:\n",
    "            continue  # Skip rows that do not have the expected structure\n",
    "\n",
    "        # Extract the field name and field content\n",
    "        field_name = data[0]\n",
    "        field_content = data[1]\n",
    "\n",
    "        # Extract the text inside both field_name and field_content\n",
    "        key = field_name.text.replace(\":\", \"\").strip()  # Field name (key) with formatting\n",
    "\n",
    "        # Extract values; handle multiple values if they exist\n",
    "        if field_content.find_all(\"a\"):\n",
    "            values = [i.text for i in field_content.find_all(\"a\")]\n",
    "        else:\n",
    "            values = field_content.text.strip()\n",
    "\n",
    "        data_dictionary[key] = values\n",
    "\n",
    "    return data_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9132d8",
   "metadata": {},
   "source": [
    "# main():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3661b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "def main(url, name):\n",
    "    \"\"\"\n",
    "    Main function to scrape data from paginated pages and save it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the initial page to start scraping from.\n",
    "    name (str): The name to use for the output CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing all the extracted data from the pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start the WebDriver with download restrictions\n",
    "    driver = open_url_block_dw(url)\n",
    "\n",
    "    # Create an empty DataFrame to store the extracted information\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Initialize a counter to track the current page number\n",
    "    n = 1\n",
    "\n",
    "    # Get the header text to determine if there is a next page\n",
    "    header = driver.find_element(By.CLASS_NAME, \"panel-heading.text-center\").text\n",
    "\n",
    "    while \"Siguiente\" in header:\n",
    "        \n",
    "        print(f\"Page {n}\")\n",
    "\n",
    "        # Execute the function to target and extract paper information\n",
    "        driver, df1 = target_papers(driver)\n",
    "\n",
    "        # Concatenate the new data with the existing DataFrame\n",
    "        df = pd.concat([df, df1], axis=0)\n",
    "\n",
    "        # Navigate to the next page\n",
    "        driver = next_page(driver)\n",
    "\n",
    "        # Increment the page counter and update the header text\n",
    "        n += 1\n",
    "        header = driver.find_element(By.CLASS_NAME, \"panel-heading.text-center\").text\n",
    "\n",
    "    # Process the last page\n",
    "    print(f\"Page {n}\")\n",
    "    driver, df1 = target_papers(driver)\n",
    "    df = pd.concat([df, df1], axis=0)\n",
    "    \n",
    "    print(\"All pages have been scanned.\")\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(f\"{name}.csv\", sep=';', index=True)\n",
    "    \n",
    "    print(f\"DataFrame {name} successfully exported.\")\n",
    "    \n",
    "    # Quit the WebDriver session\n",
    "    driver.quit()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3548e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "All pages have been scanned.\n",
      "DataFrame probando successfully exported.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url = \"http://repositorio2.unb.br/jspui/handle/10482/45731/browse?type=ppg&order=ASC&rpp=20&value=Programa+de+P%C3%B3s-Gradua%C3%A7%C3%A3o+em+Ci%C3%AAncias+Sociais+-+Estudos+Comparados+sobre+as+Am%C3%A9ricas\"\n",
    "\n",
    "name = \"probando\"\n",
    "\n",
    "df = main(url, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef01a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
